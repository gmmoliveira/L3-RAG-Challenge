{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483c7611",
   "metadata": {},
   "source": [
    "# Implementação de Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Autor: ***Guilherme Oliveira***\n",
    "### Contato: gmmoliveira1@gmail.com\n",
    "### Data: 16 de agosto de 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7ce2c4",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.document.chunking.agentic import AgenticChunking\n",
    "from agno.embedder.ollama import OllamaEmbedder\n",
    "from agno.knowledge.pdf import PDFKnowledgeBase, PDFReader\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.vectordb.pgvector import PgVector, SearchType\n",
    "from ollama import AsyncClient\n",
    "import yaml\n",
    "import asyncio\n",
    "from textwrap import dedent\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bf809",
   "metadata": {},
   "source": [
    "#### Definição de Constantes\n",
    "\n",
    "Define constantes que controlam o funcionamento geral do script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be9d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE_PATH = \"recursos/base_de_conhecimentos_PDFs/\"\n",
    "BASE_MODEL = \"qwen3:32b\"\n",
    "DATABASE_CONFIG_PATH = \"recursos/configs/database.yaml\"\n",
    "REQUIREMENTS_PATH = \"recursos/requirements.txt\"\n",
    "OLLAMA_HOST = \"http://localhost:54256\"\n",
    "QUESTIONS_PATH = \"recursos/sample_questions.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cae6a7",
   "metadata": {},
   "source": [
    "Lê o arquivo de configurações e define a constante que determina como se conectar ao banco de dados vetorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0e81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATABASE_CONFIG_PATH, 'r') as file:\n",
    "    database_config_aux = yaml.safe_load(file)\n",
    "database_config = database_config_aux[\"database\"]\n",
    "\n",
    "# postgresql+psycopg://<username>:<password>@<host>:<port>/<database>\n",
    "DATABASE_URL = f\"postgresql+psycopg://{database_config['user']}:{database_config['password']}@{database_config['host']}:{database_config['port']}/{database_config['dbname']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c9c03",
   "metadata": {},
   "source": [
    "#### Consolidação dos Requisitos Python\n",
    "\n",
    "Gera o arquivo `requirements.txt` para permitir reproduzir os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9866dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > $REQUIREMENTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8c224",
   "metadata": {},
   "source": [
    "#### Download do LLM desejado\n",
    "\n",
    "Faz download do LLM escolhido utilizando o ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39ae4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 3291abe70f16: 100% ▕██████████████████▏  20 GB                         \u001b[K\n",
      "pulling ae370d884f10: 100% ▕██████████████████▏ 1.7 KB                         \u001b[K\n",
      "pulling d18a5cc71b84: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling cff3f395ef37: 100% ▕██████████████████▏  120 B                         \u001b[K\n",
      "pulling afdf5c7585b3: 100% ▕██████████████████▏  488 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull $BASE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2503032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Dropping collection                                                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Dropping collection                                                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Table <span style=\"color: #008000; text-decoration-color: #008000\">'ai.pdf_documents'</span> dropped successfully.                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Table \u001b[32m'ai.pdf_documents'\u001b[0m dropped successfully.                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Creating collection                                                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Creating collection                                                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Loading knowledge base                                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Loading knowledge base                                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Reading: Which Economic Tasks are Performed with AI_ Evidence from Millions of Claude Conversations           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Reading: Which Economic Tasks are Performed with AI_ Evidence from Millions of Claude Conversations           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Upserted batch of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> documents.                                                                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Upserted batch of \u001b[1;36m18\u001b[0m documents.                                                                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Added <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> documents to knowledge base                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Added \u001b[1;36m18\u001b[0m documents to knowledge base                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cria a instância a classe que transforma texto em embeddings\n",
    "embedder = OllamaEmbedder(\n",
    "    dimensions=5120, # ajustar em acordo com o LLM escolhido\n",
    "    id=BASE_MODEL,\n",
    ")\n",
    "# cria a base de dados do PostgreSQL empoderada com busca vetorial\n",
    "database = PgVector(\n",
    "    table_name=\"pdf_documents\",\n",
    "    db_url=DATABASE_URL,\n",
    "    search_type=SearchType.hybrid,\n",
    "    embedder=embedder,\n",
    ")\n",
    "# Cria a instância do leitor de PDFs para consumir o artigo \n",
    "reader = PDFReader(\n",
    "    split_on_pages=False,\n",
    "    chunk=True,\n",
    ")\n",
    "# Cria a instância que determina como dividir o arquivo PDF\n",
    "chunking_strategy = AgenticChunking(\n",
    "    model=BASE_MODEL,\n",
    "    max_chunk_size=5000,\n",
    ")\n",
    "# Cria a instância da base de conhecimento\n",
    "pdf_knowledge_base = PDFKnowledgeBase(\n",
    "    path=KNOWLEDGE_BASE_PATH,\n",
    "    vector_db=database,\n",
    "    reader=reader,\n",
    "    chunking_strategy=chunking_strategy,\n",
    "    num_documents=15,\n",
    ")\n",
    "# Carrega os dados do PDF no banco de dados\n",
    "pdf_knowledge_base.load(\n",
    "    recreate=True,\n",
    "    upsert=True,\n",
    "    skip_existing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4ba83",
   "metadata": {},
   "source": [
    "Foram criados 18 fragmentos para o artigo que será utilizado como base de conhecimento\n",
    "\n",
    "#### Instanciação do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64157c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma forma de acessar o LLM via ollama de maneira assíncrona\n",
    "async_client = AsyncClient(\n",
    "    host=OLLAMA_HOST,\n",
    "    headers={\n",
    "        \"temperature\": \"0.15\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ba8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma instância do LLM que utiliza como base um único servidor\n",
    "model = Ollama(\n",
    "    id=BASE_MODEL,\n",
    "    async_client=async_client,\n",
    ")\n",
    "# Cria uma instância do agente que irá utilizar o LLM\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    knowledge=pdf_knowledge_base,\n",
    "    description=dedent(\"\"\"\n",
    "        You are a **Search-Based Research Agent**, an expert in retrieving and synthesizing the most current,\n",
    "        accurate information from trusted sources. Your core function is to answer user queries\n",
    "        exclusively using data obtained through real-time search tool calls. You must never rely\n",
    "        on pre-trained knowledge, assumptions, or unsourced information. Prioritize credibility,\n",
    "        recency, and relevance in all responses.\n",
    "    \"\"\"),\n",
    "    instructions=[\n",
    "        dedent(\"\"\"\n",
    "        1. **Mandatory Search Activation**:  \n",
    "            - For **every** user query, invoke the search tool immediately.  \n",
    "            - Generate 1–3 optimized search queries targeting credible sources (e.g., academic journals, official reports, reputable news).  \n",
    "            *Example: Querying \"peer-reviewed definition of quantum entanglement\" instead of \"what is quantum entanglement?\"* \n",
    "        \"\"\"),\n",
    "        dedent(\"\"\"\n",
    "        2. **Information Synthesis**:  \n",
    "            - Extract **only** facts from the top 3–5 search results. Cross-verify overlapping information across sources.  \n",
    "            - Discard conflicting/low-credibility data (e.g., unverified forums, outdated pages).  \n",
    "        \"\"\"),\n",
    "        dedent(\"\"\"\n",
    "        3. **Response Structure**:  \n",
    "            - **Attribution**: Cite sources for every claim. Format: `[Source: Domain/Title]`.  \n",
    "            - **Conciseness**: Answer directly in ≤3 sentences.  \n",
    "            - **Uncertainty Handling**: If sources are inadequate, respond:  \n",
    "                > \"I found no verified sources on this topic. Refine your query or ask another question.\"  \n",
    "        \"\"\"),\n",
    "        dedent(\"\"\"\n",
    "        4. **Prohibitions**:  \n",
    "            - No speculation, opinions, or unsupported statements.  \n",
    "            - No use of internal knowledge without search validation.  \n",
    "        \"\"\"),\n",
    "        dedent(\"\"\"\n",
    "        5. **Language**:\n",
    "            - Answer using the same language as the user is using in their queries. When necessary, keep technical terms in english (e.g., Retrieval-Augmented Generation---RAG).\n",
    "        \"\"\"),\n",
    "        dedent(\"\"\"\n",
    "        ### Example Interaction  \n",
    "            **User**: Define \"neuromorphic computing.\"  \n",
    "            **Agent**:  \n",
    "            1. *Searches*: [\"neuromorphic computing definition academic\"], [\"neuromorphic vs traditional architecture peer-reviewed\"].  \n",
    "            2. *Synthesizes*:  \n",
    "            > \"Neuromorphic computing designs hardware to mimic the brain’s neural structure for energy-efficient AI processing [Source: Nature Electronics]. It uses spiking neural networks for real-time learning [Source: IEEE Spectrum].\"  \n",
    "        \"\"\"),\n",
    "        dedent(\"\"\"\n",
    "        **Key Principles**:  \n",
    "            - **Search-First**: All answers originate from tool-retrieved data.  \n",
    "            - **Precision > Creativity**: Prioritize factual accuracy over engagement.  \n",
    "            - **Source Transparency**: Always expose origins for user verification.\n",
    "            - **User Language Matching**: Answer in the same language the user uses (e.g., Portuguese).\n",
    "        \"\"\"),\n",
    "    ],\n",
    "    search_knowledge=True,\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1a26a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qual é o principal objetivo do estudo conduzido pela Anthropic?',\n",
       " 'Quais são as duas categorias de tarefas que concentram quase metade do uso de IA?',\n",
       " 'Que porcentagem de ocupações usa IA para pelo menos 25% de suas tarefas associadas?',\n",
       " 'Como os autores categorizam os padrões de uso entre automação e aumento (augmentation)?',\n",
       " 'Quais habilidades ocupacionais são mais prevalentes nas conversas com IA?',\n",
       " 'Como o uso de IA varia conforme o salário das ocupações?',\n",
       " 'Qual é a principal limitação dos dados utilizados no estudo?',\n",
       " 'Como os modelos Claude 3 Opus e Claude 3.5 Sonnet diferem nos padrões de uso?',\n",
       " 'Qual framework teórico fundamenta a análise das tarefas econômicas?',\n",
       " 'Que tipo de ocupações apresenta menor penetração de IA segundo o estudo?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(QUESTIONS_PATH, 'r') as f:\n",
    "    questions = json.load(f)[\"questions\"]\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9702f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d02c48ecd6462ab94fa5906fcb78a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd40efbba7846a18e11b759575cf608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Error connecting to Infinity server at <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:7997:</span> <span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">111</span><span style=\"font-weight: bold\">]</span> Connection refused              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Error connecting to Infinity server at \u001b[4;94mhttp://localhost:7997:\u001b[0m \u001b[1m[\u001b[0mErrno \u001b[1;36m111\u001b[0m\u001b[1m]\u001b[0m Connection refused              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO</span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> documents                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO\u001b[0m Found \u001b[1;36m15\u001b[0m documents                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c20f6df76734738b2ad6c7b5a2dfb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Gera 1 resposta por vez e apresenta na tela via streamming\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkdown\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Permite gerar a resposta assíncrona, porém não favorece a visualização\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m#asyncio.run(agent.print_response(question, markdown=True))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/agno/agent/agent.py:7086\u001b[39m, in \u001b[36mAgent.print_response\u001b[39m\u001b[34m(self, message, session_id, session_state, user_id, messages, audio, images, videos, files, stream, stream_intermediate_steps, markdown, show_message, show_reasoning, show_full_reasoning, console, tags_to_include_in_markdown, knowledge_filters, **kwargs)\u001b[39m\n\u001b[32m   7083\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[32m   7084\u001b[39m     live_log.update(Group(*panels))\n\u001b[32m-> \u001b[39m\u001b[32m7086\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7089\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7090\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7091\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7092\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7093\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7094\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7095\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   7097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mknowledge_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknowledge_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7099\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7100\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   7101\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunResponseEvent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   7102\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_paused\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/agno/agent/agent.py:888\u001b[39m, in \u001b[36mAgent._run_stream\u001b[39m\u001b[34m(self, run_response, run_messages, session_id, user_id, response_format, stream_intermediate_steps, refresh_session_before_write)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# 2. Process model response\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_model_response_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/agno/agent/agent.py:3066\u001b[39m, in \u001b[36mAgent._handle_model_response_stream\u001b[39m\u001b[34m(self, run_response, run_messages, response_format, stream_intermediate_steps)\u001b[39m\n\u001b[32m   3063\u001b[39m     log_debug(\u001b[33m\"\u001b[39m\u001b[33mResponse model set, model response is not streamed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3064\u001b[39m     stream_model_response = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3066\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_event\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tools_for_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_functions_for_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_call_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_call_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_model_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_model_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_model_response_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[38;5;66;03m# Determine reasoning completed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/agno/models/base.py:762\u001b[39m, in \u001b[36mModel.response_stream\u001b[39m\u001b[34m(self, messages, response_format, tools, functions, tool_choice, tool_call_limit, stream_model_response)\u001b[39m\n\u001b[32m    759\u001b[39m stream_data = MessageData()\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_model_response:\n\u001b[32m    761\u001b[39m     \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response_stream(\n\u001b[32m    763\u001b[39m         messages=messages,\n\u001b[32m    764\u001b[39m         assistant_message=assistant_message,\n\u001b[32m    765\u001b[39m         stream_data=stream_data,\n\u001b[32m    766\u001b[39m         response_format=response_format,\n\u001b[32m    767\u001b[39m         tools=tools,\n\u001b[32m    768\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tool_choice,\n\u001b[32m    769\u001b[39m     )\n\u001b[32m    771\u001b[39m     \u001b[38;5;66;03m# Populate assistant message from stream data\u001b[39;00m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_data.response_content:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/agno/models/base.py:724\u001b[39m, in \u001b[36mModel.process_response_stream\u001b[39m\u001b[34m(self, messages, assistant_message, stream_data, response_format, tools, tool_choice)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[33;03mProcess a streaming response from the model.\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    723\u001b[39m assistant_message.metrics.start_timer()\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_response_delta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_provider_response_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_populate_stream_data_and_assistant_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_response_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response_delta\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/agno/models/ollama/chat.py:228\u001b[39m, in \u001b[36mOllama.invoke_stream\u001b[39m\u001b[34m(self, messages, response_format, tools, tool_choice)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke_stream\u001b[39m(\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    220\u001b[39m     messages: List[Message],\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m     tool_choice: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    224\u001b[39m ) -> Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    225\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[33;03m    Sends a streaming chat request to the Ollama API.\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_client().chat(\n\u001b[32m    229\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    230\u001b[39m         messages=[\u001b[38;5;28mself\u001b[39m._format_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    231\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    232\u001b[39m         **\u001b[38;5;28mself\u001b[39m.get_request_params(tools=tools),\n\u001b[32m    233\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/ollama/_client.py:172\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    169\u001b[39m   e.response.read()\n\u001b[32m    170\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpx/_models.py:863\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    861\u001b[39m decoder = LineDecoder()\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpx/_models.py:850\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    848\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpx/_models.py:831\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    829\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpx/_models.py:885\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    882\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpx/_client.py:127\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpx/_transports/default.py:116\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/L3-RAG-Challenge-main/venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for question in questions:\n",
    "    # Gera 1 resposta por vez e apresenta na tela via streamming\n",
    "    agent.print_response(question, stream=True, markdown=True)\n",
    "    # Permite gerar a resposta assíncrona, porém não favorece a visualização\n",
    "    #asyncio.run(agent.print_response(question, markdown=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
